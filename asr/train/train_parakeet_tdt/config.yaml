# NVIDIA Parakeet TDT 0.6B v3 Fine-tuning Configuration
# Optimized for ASR fine-tuning on custom datasets

model:
  # Pre-trained model from HuggingFace/NVIDIA
  name: "nvidia/parakeet-tdt-0.6b-v3"
  # Alternative models:
  # - "nvidia/parakeet-tdt-1.1b" (more accurate, slower)
  # - "nvidia/parakeet-rnnt-0.6b" (streaming-capable)

data:
  # NeMo manifest files (JSONL format)
  # Each line: {"audio_filepath": "/path/to/audio.wav", "text": "transcription", "duration": 2.5}
  # These are generated by prepare_data.py into ./data/ directory
  train_manifest: "./data/train_manifest.json"
  val_manifest: "./data/val_manifest.json"
  
  # Dataset size (standardized with Qwen2.5-Omni)
  max_samples: -1  # Use full dataset (~5.2M samples)
  # max_samples: 500000  # Limit training samples (set to null or -1 for full dataset)
  # max_samples: 1000000  # Use 1M samples for extended training (requires more memory)
  
  # Audio processing
  sampling_rate: 16000
  max_audio_length: 30.0  # seconds
  min_audio_length: 0.1   # seconds

training:
  # Output directory
  output_dir: "./outputs/parakeet-tdt-malay-asr"
  run_name: "parakeet-tdt-malay-finetuning"
  
  # Training epochs and steps
  num_train_epochs: 1.0  # Standardized with Qwen2.5-Omni config
  max_steps: -1  # Set to positive number to limit total steps
  
  # Batch size and accumulation (adjusted for memory constraints)
  # Parakeet 0.6B with large dataset (1M samples) needs aggressive memory optimization
  # Effective batch size = 8 * 16 = 128 (same as Qwen2.5-Omni)
  per_device_train_batch_size: 8    # Further reduced to avoid OOM with 1M samples
  per_device_eval_batch_size: 4     # Further reduced to avoid OOM during evaluation
  gradient_accumulation_steps: 16   # Increased to maintain effective batch size of 128
  
  # Optimizer settings (standardized with Qwen2.5-Omni)
  optimizer: "adamw_8bit"  # 8-bit optimizer: 75% memory reduction, ~5% slower
  # optimizer: "adamw8bit" # Alternative naming (also works)
  # optimizer: "adamw"     # Standard optimizer (if bitsandbytes unavailable)
  learning_rate: 2.0e-4  # Standardized (Qwen2.5-Omni working config)
  weight_decay: 0.0001
  max_grad_norm: 1.0
  
  # Learning rate scheduler (standardized with Qwen2.5-Omni)
  scheduler: "CosineAnnealing"
  warmup_steps: 100      # Standardized with Qwen2.5-Omni
  min_learning_rate: 1.0e-6
  
  # Precision and optimization (standardized with Qwen2.5-Omni)
  fp16: false  # Disabled in favor of bf16
  bf16: true   # A100/H100 optimized (standardized with Qwen2.5-Omni)
  
  # Logging and evaluation (optimized for full dataset training)
  logging_steps: 100     # Log frequently for monitoring
  eval_steps: 6700       # Validate every 6.7k steps (~2x per day, ~12 hours)
  save_steps: 6700       # Save every 6.7k steps (~2x per day, ~12 hours)
  save_total_limit: 3    # Keep last 3 checkpoints
  load_best_model_at_end: true  # Load best model at end
  
  # DataLoader settings (increased for better throughput)
  dataloader_num_workers: 8  # Increased for faster data loading
  dataloader_pin_memory: true
  
  # Resume from checkpoint
  resume_from_checkpoint: false
  
  # Hardware
  num_gpus: 1  # Multi-GPU not fully tested yet

# Weights & Biases (optional)
wandb:
  enabled: false
  project: "parakeet-tdt-malay-asr"
  entity: null  # Your W&B username/team
  run_name: "parakeet-tdt-v3-finetuning"

# Notes:
# 1. Config optimized for FULL DATASET training (~5.2M samples, ~3 days)
# 2. Effective batch size: 8 * 16 = 128 (same as Qwen2.5-Omni)
# 3. Learning rate 2e-4 proven working for Malaysian ASR (Qwen2.5-Omni)
# 4. GPU utilization: ~35-40GB with 8-bit optimizer (vs ~45-50GB with standard optimizer)
# 5. 8-bit optimizer: Requires `pip install bitsandbytes` (auto-falls back if unavailable)
# 6. Batch sizes: train=8, eval=4, grad_accum=16 (optimized for memory)
# 7. Total steps: ~40,625 steps (5.2M samples / 128 batch size)
# 8. Throughput: ~558 steps/hour (based on empirical 500k/7hr measurement)
# 9. Checkpoints saved every 6,700 steps (~2x per day, ~12 hours)
# 10. Expected completion: ~73 hours (~3 days) based on actual throughput
# 11. Memory optimization: PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True (set in run_training.sh)
# 12. Model automatically adds punctuation and capitalization
# 13. bf16 recommended for A100/H100 (better than fp16)

