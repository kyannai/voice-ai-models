# NVIDIA Parakeet TDT 0.6B Multilingual Overfit Configuration
# Sanity check: overfit a tiny subset to verify pipeline correctness.

model:
  # Fixed model with proper tokenizer configuration
  name: "/home/kyan/voice-ai/asr/train/models/parakeet-tdt-multilingual-init.nemo"
  gradient_checkpointing: true

data:
  # Same manifests, but we cap samples for overfit
  train_manifest: "../training_data/multilingual-overfit/manifests/train_manifest.json"
  val_manifest: "../training_data/multilingual-overfit/manifests/val_manifest.json"
  # Use slightly more samples to provide more diverse gradients
  max_samples: 32
  max_val_samples: 16
  sampling_rate: 16000
  max_audio_length: 30.0
  min_audio_length: 0.1

training:
  output_dir: "./outputs/parakeet-tdt-multilingual-overfit"
  run_name: "parakeet-tdt-multilingual-overfit"

  # Epochs for overfit test
  num_train_epochs: 100
  max_steps: -1

  per_device_train_batch_size: 2  # Smaller batch for more gradient updates
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 1

  # For overfit test: let entire model learn (encoder + decoder + joint)
  # This allows the encoder to adapt to new languages
  freeze_encoder: false

  optimizer: "adamw"
  # Higher LR for overfit test - new tokens need stronger gradients
  learning_rate: 5.0e-4
  weight_decay: 0.0
  max_grad_norm: 1.0  # Allow larger gradients for new token learning

  # Learning rate scheduler
  scheduler: "CosineAnnealing"
  warmup_steps: 10  # Very short warmup for overfit test
  min_learning_rate: 1.0e-6

  fp16: false
  bf16: true

  logging_steps: 10
  # Eval once per epoch (8 steps) - don't waste time on frequent validation
  eval_steps: 8
  save_steps: 0
  save_total_limit: 0
  load_best_model_at_end: false
  disable_checkpointing: true
  disable_spec_augment: true

  dataloader_num_workers: 4
  dataloader_pin_memory: true

  resume_from_checkpoint: false
  num_gpus: 1
