# Parakeet TDT Continued Training with KeSpeech
# ==============================================
# Continue training from Malaysian STT checkpoint with KeSpeech (~31h Mandarin from HuggingFace)
#
# Usage:
#   1. First prepare KeSpeech data:
#      cd /home/kyan/voice-ai/asr/train && make prepare-kespeech
#
#   2. Then train from checkpoint:
#      cd train_parakeet_tdt
#      make train-kespeech

model:
  # Base model (weights will be overwritten by checkpoint)
  name: "nvidia/parakeet-tdt-0.6b-v3"
  
  # Memory optimization
  gradient_checkpointing: true

data:
  # KeSpeech manifests (generated by: make prepare-kespeech)
  train_manifest: "../training_data/kespeech/data/manifests/train_manifest.json"
  val_manifest: "../training_data/kespeech/data/manifests/val_manifest.json"
  
  # Dataset size (~31h from HuggingFace test split)
  max_samples: -1  # Use all available samples (~18.7k train, ~1k val)
  max_val_samples: -1  # Use all validation samples
  
  # Audio processing
  sampling_rate: 16000
  max_audio_length: 30.0
  min_audio_length: 0.1

training:
  # Output directory (separate from stage1)
  output_dir: "./outputs/parakeet-tdt-kespeech"
  run_name: "parakeet-kespeech-continued"
  
  # Training duration - more epochs for smaller dataset
  num_train_epochs: 10
  max_steps: -1
  
  # Batch size (same as stage1)
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 16  # Effective batch size: 128
  
  # Lower learning rate for continued training
  optimizer: "adamw_8bit"
  learning_rate: 5.0e-5  # Reduced from 2e-4 for continued training
  weight_decay: 0.0001
  max_grad_norm: 1.0
  
  # Learning rate scheduler
  scheduler: "CosineAnnealing"
  warmup_steps: 100  # Fewer warmup steps for smaller dataset
  min_learning_rate: 1.0e-6
  
  # Precision
  fp16: false
  bf16: true
  
  # Logging and evaluation (adjusted for ~2342 batches/epoch)
  logging_steps: 50
  eval_steps: 500  # Eval every 500 steps (~4-5x per epoch)
  save_steps: 500
  save_total_limit: 5
  load_best_model_at_end: true
  
  # DataLoader
  dataloader_num_workers: 8
  dataloader_pin_memory: true
  
  # Checkpoint handling:
  # - checkpoint_path: Initial checkpoint to start from (only used if no checkpoint exists in output_dir)
  # - resume_from_checkpoint: true enables auto-resume from output_dir if training is interrupted
  resume_from_checkpoint: true
  checkpoint_path: "/home/kyan/voice-ai/asr/train/train_parakeet_tdt/models/parakeet-tdt--epoch=01-step=41220-val_wer=0.2038-last.ckpt"
  
  # Hardware
  num_gpus: 1

wandb:
  enabled: false
  project: "parakeet-tdt-kespeech"
  run_name: "kespeech-continued"

# Notes:
# 1. First run: starts from Malaysian checkpoint (step 41220)
# 2. If interrupted: auto-resumes from latest checkpoint in output_dir
# 3. Learning rate: 5e-5 (lower than initial 2e-4 for fine-tuning)
# 4. Warmup: 100 steps (small dataset converges quickly)
# 5. KeSpeech HuggingFace: ~31h Mandarin (~18.7k train samples)
# 6. Eval/save every 500 steps (~4-5x per epoch)
# 7. Training 10 epochs due to small dataset size
