# Quick Start: Monitoring WER During Training

## TL;DR

WER **is printed** during training, but **TensorBoard is the best way** to see it clearly.

## ğŸš€ Quick Setup (2 Steps)

### Terminal 1: Start Training
```bash
cd /path/to/train_parakeet_tdt
bash run_training.sh
```

### Terminal 2: Start TensorBoard
```bash
cd /path/to/train_parakeet_tdt
bash start_tensorboard.sh
```

**Then open:** http://localhost:6006

**Look for:** `val_wer` graph (should go DOWN ğŸ“‰)

---

## Where WER Appears

### âœ… Option 1: TensorBoard (BEST)
```bash
bash start_tensorboard.sh
```
Then go to http://localhost:6006 â†’ Scalars â†’ `val_wer`

**Pros:**
- âœ… Real-time graphs
- âœ… Easy to see trends
- âœ… Compare multiple runs
- âœ… Professional visualization

### âœ… Option 2: Console Output
During training, look for:
```
Epoch 0:  10%|â–ˆ | 1000/10000 [00:15:48<2:18:21, train_loss=1.234, val_wer=0.452]
                                                                      ^^^^^^^^^^
                                                                      HERE!
```

**When it appears:**
- Every `eval_steps` (1000 for 100k config, 10000 for full dataset)
- For 100k training: First WER appears after ~15 minutes

**Pros:**
- âœ… No extra tools needed
- âœ… Shows up automatically

**Cons:**
- âš ï¸ Easy to miss in scrolling output
- âš ï¸ Only shows current value, not trends

### âœ… Option 3: Checkpoint Filenames
```bash
ls outputs/parakeet-tdt-malay-asr/parakeet-tdt-malay-finetuning/*/checkpoints/
```

Files are named: `parakeet-tdt--epoch=00-step=1000-val_wer=0.4523.ckpt`
                                                      ^^^^^^^^^^^^^^
                                                      WER in filename!

**Pros:**
- âœ… Easy to see best checkpoint
- âœ… Persists after training

**Cons:**
- âš ï¸ Only for full dataset training (100k config has checkpoints disabled)

### âœ… Option 4: Log Files
```bash
grep "val_wer" outputs/*/parakeet-tdt-malay-finetuning/*/lightning_logs.txt
```

**Pros:**
- âœ… Complete history
- âœ… Can grep/parse

**Cons:**
- âš ï¸ Not as visual as TensorBoard

---

## What's Normal?

### 100k Sample Training

| Time | Step | Expected WER | Status |
|------|------|--------------|--------|
| Start | 0 | 0.90-1.00 | ğŸ”´ Baseline (random guesses) |
| ~15 min | 1,000 | 0.40-0.60 | ğŸŸ¡ Learning started |
| ~30 min | 2,000 | 0.30-0.40 | ğŸŸ¢ Good progress |
| ~1 hour | 4,000 | 0.20-0.30 | ğŸŸ¢ Improving nicely |
| ~2 hours | 8,000+ | 0.10-0.20 | ğŸŸ¢ Very good! |

### Full Dataset Training (5.2M samples)

| Time | Step | Expected WER | Status |
|------|------|--------------|--------|
| Start | 0 | 0.90-1.00 | ğŸ”´ Baseline |
| ~2 hours | 10,000 | 0.20-0.35 | ğŸŸ¡ Early learning |
| ~6 hours | 30,000 | 0.12-0.20 | ğŸŸ¢ Good progress |
| ~12 hours | 60,000 | 0.08-0.15 | ğŸŸ¢ Very good |
| ~24 hours | 120,000+ | 0.05-0.10 | ğŸŸ¢ Excellent! |

**Note:** WER should generally **decrease over time**. If it increases or plateaus, something might be wrong.

---

## Example Console Output

### What You'll See Every ~50 Steps
```bash
[NeMo I 2024-11-06 10:15:23] Step: 50   Loss: 2.345
[NeMo I 2024-11-06 10:15:45] Step: 100  Loss: 1.987
[NeMo I 2024-11-06 10:16:08] Step: 150  Loss: 1.654
```

### What You'll See Every ~1000 Steps (Validation)
```bash
[NeMo I 2024-11-06 10:30:45] Validation started at step 1000
[NeMo I 2024-11-06 10:31:10] val_wer: 0.4523 (45.23%)  â† HERE!
[NeMo I 2024-11-06 10:31:10] val_loss: 0.8765
```

### PyTorch Lightning Progress Bar
```
Epoch 0:  10%|â–ˆ         | 1000/10000 [15:48<2:18:21, train_loss=1.234, val_wer=0.452]
                                                                        ^^^^^^^^^^^^^
                                                                        WER shown here too!
```

---

## Troubleshooting

### "I don't see any WER"

**Check:**
1. Has validation run yet? (First validation at step 1000 for 100k config)
2. Is console output scrolling too fast?
3. Try TensorBoard instead: `bash start_tensorboard.sh`

**For 100k training:**
- First WER appears after ~15 minutes (at step 1000)
- If you started recently, just wait!

### "TensorBoard shows no data"

**Try:**
```bash
# Make sure training has started and created outputs
ls outputs/

# Restart TensorBoard
bash start_tensorboard.sh
```

### "WER is increasing or stuck"

**This could mean:**
- ğŸ”´ Learning rate too high â†’ Reduce to 1e-4
- ğŸ”´ Data quality issues â†’ Check manifests
- ğŸ”´ Model divergence â†’ Restart training

**Check training loss:**
- Should also be decreasing
- If loss is decreasing but WER isn't, wait longer

---

## Commands Cheat Sheet

### Start Everything
```bash
# Terminal 1: Training
bash run_training.sh

# Terminal 2: Monitoring
bash start_tensorboard.sh
```

### Check Latest WER
```bash
# Quick check without TensorBoard
grep "val_wer" outputs/*/parakeet-tdt-malay-finetuning/*/lightning_logs.txt | tail -5
```

### Watch Training Log Live
```bash
tail -f outputs/parakeet-tdt-malay-asr/parakeet-tdt-malay-finetuning/*/nemo_log_*.txt
```

### List All Checkpoints with WER
```bash
ls -lh outputs/*/parakeet-tdt-malay-finetuning/*/checkpoints/ | grep "val_wer"
```

---

## Summary

âœ… **WER IS logged** - Every 1000 steps (100k config) or 10000 steps (full dataset)
âœ… **Best way to see it:** TensorBoard (`bash start_tensorboard.sh`)
âœ… **Also appears in:** Console output, log files, checkpoint names
âœ… **Normal values:** Start at ~0.9, end at 0.1-0.2 (100k) or 0.05-0.1 (full dataset)
âœ… **First WER appears:** After ~15 minutes (100k config)

**Don't stress if you don't see it immediately in console - use TensorBoard!** ğŸ“Š

---

## Visual Guide

### TensorBoard Interface
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TensorBoard - localhost:6006            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  [Scalars] [Images] [Graphs] [Dist]    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                         â”‚
â”‚  val_wer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚  1.0 â”‚*                                 â”‚
â”‚      â”‚  *                               â”‚
â”‚  0.8 â”‚    *                             â”‚
â”‚      â”‚      *                           â”‚
â”‚  0.6 â”‚        *                         â”‚
â”‚      â”‚          *___                    â”‚
â”‚  0.4 â”‚              *___                â”‚
â”‚      â”‚                  *___            â”‚
â”‚  0.2 â”‚                      *___        â”‚
â”‚  0.0 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>  â”‚
â”‚       0    2k   4k   6k   8k   10k      â”‚
â”‚                                         â”‚
â”‚  â† This graph should go DOWN!           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**If the graph goes down â†’ Training is working! âœ…**
**If the graph is flat or up â†’ Something's wrong! âš ï¸**

---

For detailed information, see: **MONITORING_TRAINING.md**

