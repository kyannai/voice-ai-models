# ASR Training - Base Dependencies
# Common packages needed for all ASR training frameworks
# Updated: 2025-11-06 - Latest stable versions

# ==========================================
# CORE ML/DL FRAMEWORKS
# ==========================================
torch>=2.5.0
torchaudio>=2.5.0
transformers>=4.45.0
accelerate>=0.34.0

# LoRA and Quantization (Qwen2-Audio, Qwen2.5-Omni, Qwen3-Omni)
peft>=0.13.0
bitsandbytes>=0.44.0

# Note: Unsloth can be installed separately if needed
# pip install unsloth

# ==========================================
# NEMO FRAMEWORK (Parakeet TDT)
# ==========================================
# Note: Install NeMo from GitHub for latest features
# pip install git+https://github.com/NVIDIA/NeMo.git@main#egg=nemo_toolkit[asr]
# Or use: pip install nemo_toolkit[asr]
omegaconf>=2.3.0
hydra-core>=1.3.2
lightning>=2.0.0  # PyTorch Lightning (NeMo dependency)

# ==========================================
# AUDIO PROCESSING
# ==========================================
librosa>=0.10.2
soundfile>=0.12.1
audioread>=3.0.0  # For MP3/other format support via ffmpeg

# ==========================================
# DATA PROCESSING
# ==========================================
numpy>=1.26.0,<2.0  # NumPy 2.x may have compatibility issues
pandas>=2.2.0
datasets>=2.20.0

# ==========================================
# MULTIMODAL DEPENDENCIES (Qwen2.5-Omni, Qwen3-Omni)
# ==========================================
pillow>=10.4.0
torchvision>=0.20.0

# ==========================================
# TRAINING UTILITIES
# ==========================================
tensorboard>=2.17.0
tqdm>=4.66.0
wandb>=0.18.0  # Optional but recommended for experiment tracking

# ==========================================
# CONFIGURATION & UTILITIES
# ==========================================
pyyaml>=6.0.2

# ==========================================
# OPTIONAL: Additional Tools
# ==========================================
# mlflow>=2.10.0  # Alternative experiment tracking
# deepspeed>=0.14.0  # For distributed training optimization
# flash-attn>=2.6.0  # Flash Attention (requires CUDA)

