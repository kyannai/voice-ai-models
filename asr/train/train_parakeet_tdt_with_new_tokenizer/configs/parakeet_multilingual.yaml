# NVIDIA Parakeet TDT 0.6B Multilingual Training Configuration
# Training with new multilingual tokenizer (EN/MS/ZH)
#
# This config is for fine-tuning the modified model with:
# - 5500h Malaysian-STT (English + Malay)
# - 1500h KeSpeech (Chinese)
# Total: ~7000h multilingual ASR data

model:
  # Modified model with multilingual tokenizer
  # Generated by: python src/modify_architecture.py
  name: "./models/parakeet-tdt-0.6b-multilingual-init.nemo"

  # Memory optimization: Enable gradient checkpointing (30-50% memory savings)
  gradient_checkpointing: true

data:
  # Combined multilingual manifests (EN/MS/ZH)
  # Generated by: make prepare-multilingual (from asr/train/)
  # Uses malaysian-stt/data/manifests + KeSpeech/data/manifests
  train_manifest: "../training_data/multilingual/manifests/train_manifest.json"
  val_manifest: "../training_data/multilingual/manifests/val_manifest.json"
  
  # Dataset size: ~7000h total
  max_samples: -1  # Use full dataset
  
  # Validation set limit (speeds up eval checkpoints)
  max_val_samples: 1000  # Limit validation to 5k samples
  
  # Audio processing
  sampling_rate: 16000
  max_audio_length: 30.0  # seconds
  min_audio_length: 0.1   # seconds

training:
  # Output directory
  output_dir: "./outputs/parakeet-tdt-multilingual"
  run_name: "parakeet-tdt-multilingual-training"
  
  # Training epochs
  # For 7000h data, 5-10 epochs is reasonable
  num_train_epochs: 1
  max_steps: -1  # Set to positive number to limit total steps
  
  # Batch size and accumulation
  # Adjusted for mixed multilingual data
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 16  # Effective batch size: 8 * 16 = 128
  
  # Freeze encoder - CRITICAL for new tokenizer training
  # The encoder (608M params) is already trained, only decoder/joint need learning
  freeze_encoder: true
  
  # Optimizer settings
  # With frozen encoder + randomly initialized decoder/joint, use conservative LR
  optimizer: "adamw_8bit"
  learning_rate: 1.0e-5  # Conservative - decoder/joint learning from scratch
  weight_decay: 0.0001
  max_grad_norm: 1.0  # Can relax since only training ~23M params
  
  # Learning rate scheduler
  # Longer warmup helps decoder/joint find stable gradients
  scheduler: "CosineAnnealing"
  warmup_steps: 10000  # Longer warmup for randomly initialized layers
  min_learning_rate: 1.0e-7
  
  # Precision and optimization
  fp16: false
  bf16: true  # A100/H100 optimized
  
  # Logging and evaluation
  logging_steps: 100
  eval_steps: 41000  # Validate every 10k steps
  save_steps: 41000  # Save every 10k steps
  save_total_limit: 5  # Keep more checkpoints for long training
  load_best_model_at_end: true
  
  # DataLoader settings
  dataloader_num_workers: 8
  dataloader_pin_memory: true
  
  # Resume from checkpoint
  resume_from_checkpoint: true
  
  # Hardware
  num_gpus: 1

# Notes:
# 1. This config is for training with the NEW multilingual tokenizer
# 2. The base model's decoder and joint layers are re-initialized for new vocab
# 3. Encoder weights are FROZEN - only decoder/joint are trained (~23M params)
# 4. freeze_encoder=true prevents training instability with mismatched vocab
# 5. Learning rate 5e-5 is safe with frozen encoder
# 6. Expected training time: ~60 hours on A100 GPU for 1 epoch
# 7. Data mixing: 78% MS/EN, 22% Chinese (natural ratio)
#
# Training stages recommendation:
# Stage 1: Train with freeze_encoder=true until WER improves significantly
# Stage 2: Set freeze_encoder=false, lower LR to 1e-5, fine-tune entire model
