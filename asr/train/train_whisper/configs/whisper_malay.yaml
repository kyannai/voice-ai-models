# Whisper Large-v3-Turbo Fine-tuning for Malay ASR
# =================================================
# Fine-tune OpenAI Whisper on Malaysian STT data for improved
# Malay code-switching accuracy.
#
# Uses LoRA (Low-Rank Adaptation) to train only the decoder,
# preserving English and Chinese capabilities while improving Malay.

model:
  # Pre-trained model from HuggingFace
  name: "openai/whisper-large-v3-turbo"
  # Alternative models:
  # - "openai/whisper-large-v3" (more accurate, slower)
  # - "openai/whisper-medium" (faster, less accurate)
  # - "mesolitica/Malaysian-whisper-large-v3-turbo-v3" (already fine-tuned for Malaysian)

  # Language to force during training (None = auto-detect)
  # For Malaysian data, we train with mixed languages (ms, en, zh)
  language: null
  
  # Task: transcribe or translate
  task: "transcribe"

# LoRA Configuration
# Trains only small adapter layers (~0.5% of parameters)
# Keeps base model frozen to preserve English/Chinese capabilities
lora:
  enabled: true
  
  # LoRA hyperparameters
  r: 32                    # Rank of the low-rank matrices (higher = more capacity)
  lora_alpha: 64           # Scaling factor (typically 2x r)
  lora_dropout: 0.05       # Dropout for regularization
  
  # Target modules to apply LoRA (DECODER ONLY via regex)
  # Encoder is automatically frozen (no LoRA applied)
  # Decoder adapts for language-specific Malay improvements
  # Use regex pattern: ".*decoder.*module_name" to match decoder only
  target_modules: ".*decoder.*(q_proj|v_proj|k_proj|out_proj|fc1|fc2)"
  
  # Which modules to train LoRA on
  modules_to_save: null    # Additional modules to fully train (null = none)
  
  # Freeze encoder - not needed when LoRA only targets decoder
  freeze_encoder: false

data:
  # HuggingFace dataset format (JSONL with audio_filepath, text, duration)
  # Generated by: make prepare-data
  train_manifest: "/home/kyan/voice-ai/asr/train/training_data/malaysian-stt/data/manifests/train_manifest.json"
  val_manifest: "/home/kyan/voice-ai/asr/train/training_data/malaysian-stt/data/manifests/val_manifest.json"
  
  # Dataset size limits
  max_samples: null  # Use full dataset (null or -1 for all)
  # max_samples: 100000  # Limit for faster experimentation
  
  max_val_samples: 1000  # Limit validation for faster eval
  
  # Audio processing
  sampling_rate: 16000
  max_audio_length: 20.0  # seconds - Whisper max is 30s
  min_audio_length: 2   # seconds

training:
  # Output directory
  output_dir: "./outputs/whisper-malay"
  run_name: "whisper-large-v3-turbo-malay"
  
  # Training epochs and steps
  num_train_epochs: 1
  max_steps: -1  # Set to positive number to limit total steps
  
  # Batch size and accumulation
  # Decoder-only LoRA uses less memory, push batch higher
  per_device_train_batch_size: 384
  per_device_eval_batch_size: 64
  gradient_accumulation_steps: 1  # Effective batch size: 384 * 1 = 384
  
  # Optimizer settings
  optimizer: "adamw_torch"  # adamw_torch, adamw_hf, adafactor
  learning_rate: 5.0e-5     # Conservative LR for LoRA to prevent divergence
  weight_decay: 0.01
  max_grad_norm: 1.0
  
  # Learning rate scheduler
  lr_scheduler_type: "linear"  # linear, cosine, cosine_with_restarts
  warmup_steps: 500
  warmup_ratio: 0.0  # Alternative to warmup_steps (0.1 = 10% of training)
  
  # Precision and optimization
  fp16: false  # Set true if no bf16 support
  bf16: true   # Recommended for A100/H100/newer GPUs
  
  # Gradient checkpointing (memory savings, ~20% slower)
  # Required even with LoRA due to activation memory
  gradient_checkpointing: true
  
  # Logging and evaluation (optimized for full dataset training) 
  logging_steps: 100     # Log frequently for monitoring
  eval_steps: 100
  save_steps: 100
  save_total_limit: 3    # Keep last 3 checkpoints

  
  # Evaluation strategy
  evaluation_strategy: "steps"
  save_strategy: "steps"
  load_best_model_at_end: false
  metric_for_best_model: "wer"
  greater_is_better: false  # Lower WER is better
  
  # DataLoader settings
  dataloader_num_workers: 8  # More workers for faster data loading
  dataloader_pin_memory: true
  remove_unused_columns: false  # Important for custom datasets
  
  # Resume from checkpoint
  resume_from_checkpoint: latest  # Path to checkpoint or "latest"
  
  # Hardware
  num_gpus: 1

# Generation settings for evaluation
generation:
  max_new_tokens: 448     # Whisper default
  num_beams: 1            # Greedy decoding (faster)
  # num_beams: 5          # Beam search (more accurate, slower)

# Weights & Biases (optional)
wandb:
  enabled: false
  project: "whisper-malay"
  entity: null
  run_name: "whisper-large-v3-turbo-malay"

# Notes:
# 1. Whisper large-v3-turbo has 809M parameters (reduced decoder layers)
# 2. Effective batch size: 80 (balanced speed + accuracy)
# 3. Learning rate 1e-5 is conservative for fine-tuning
# 4. bf16 recommended for modern GPUs (A100/H100/4090)
# 5. Gradient checkpointing trades compute for memory (~30% savings)
# 6. WER (Word Error Rate) is the primary metric
# 7. Malaysian-STT dataset: EN+MS only, preprocessed for ASR
# 8. A100 80GB: batch_size=80 + grad checkpointing = ~32GB
# 9. Expected training time: ~53 hours for 1 epoch
#
# LoRA Notes:
# 10. LoRA trains ~0.5% of parameters, keeping base model frozen
# 11. Encoder frozen = acoustic features preserved (language-agnostic)
# 12. Decoder LoRA = language-specific adaptation for Malay
# 13. English and Chinese capabilities preserved in frozen base weights
# 14. Faster training and less memory than full fine-tuning