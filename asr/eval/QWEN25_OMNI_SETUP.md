# Qwen2.5-Omni Setup Complete! üéâ

## ‚úÖ What's Been Added

### 1. **New Transcriber Script**
- `transcribe/transcribe_qwen25omni.py` - Specialized for Qwen2.5-Omni-7B
- Automatically detected by `evaluate.py` (no `--framework` needed!)
- Flash-Attention 2 support (optional, auto-detected)
- Official code patterns followed exactly

### 2. **Key Optimizations**
- ‚úÖ Flash-Attention 2 detection (uses if available, fallback if not)
- ‚úÖ `model.disable_talker()` - Saves ~2GB GPU memory
- ‚úÖ `torch_dtype="auto"` - Follows official recommendation
- ‚úÖ `device_map="auto"` - Optimal device placement
- ‚úÖ No audio generation for ASR-only tasks

### 3. **Documentation Updated**
- `README.md` now includes comprehensive Qwen2.5-Omni section
- Performance comparison table
- Installation instructions for Flash-Attention 2
- Usage examples

## üöÄ How to Use (Server Side)

### Step 1: Upload Files to Server

Upload these 3 files to your server at `/home/kyan/voice-ai/asr/eval/`:

1. **`evaluate.py`** (updated detection logic)
2. **`transcribe/transcribe_qwen25omni.py`** (new script)
3. **`README.md`** (updated docs)

### Step 2: Clear Downloaded Model (Optional)

The model you downloaded earlier was trying to use Flash-Attention 2 but failed. Clear it:

```bash
# On server
rm -rf ~/.cache/huggingface/hub/models--Qwen--Qwen2.5-Omni-7B
```

### Step 3: Install Flash-Attention 2 (Recommended but Optional)

For **2-3x faster inference**, install Flash-Attention 2:

```bash
# On server (in your eval environment)
cd /home/kyan/voice-ai/asr/eval
source .venv/bin/activate  # or: conda activate eval

pip install flash-attn --no-build-isolation
```

**Note:** If flash-attn installation fails, that's OK! The script will auto-detect and fall back to standard attention.

### Step 4: Run Evaluation

```bash
# On server
cd /home/kyan/voice-ai/asr/eval
source .venv/bin/activate

# Full evaluation
python evaluate.py \
  --model Qwen/Qwen2.5-Omni-7B \
  --test-data test_data/ytl-malay-test/asr_ground_truths.json \
  --audio-dir test_data/ytl-malay-test \
  --asr-prompt "Transcribe this Malay audio accurately, preserving all English words and discourse particles. Return only the transcribed text with no preamble or explanation."

# Quick test (10 samples)
python evaluate.py \
  --model Qwen/Qwen2.5-Omni-7B \
  --test-data test_data/ytl-malay-test/asr_ground_truths.json \
  --audio-dir test_data/ytl-malay-test \
  --asr-prompt "Transcribe the audio into text." \
  --max-samples 10
```

## üìä Expected Performance

| Metric | Value |
|--------|-------|
| **Model Size** | 7B parameters (~14GB GPU) |
| **Speed (A100)** | ~0.2-0.5s/sample (with Flash-Attn 2) |
| **Speed (no Flash-Attn)** | ~0.5-1.0s/sample |
| **LibriSpeech WER** | 1.6 (dev) / 3.4 (test) |
| **Memory** | ~14GB GPU (talker disabled) |

## üîß Troubleshooting

### If you see: "flash_attn seems to be not installed"
‚úÖ **FIXED!** The script now auto-detects Flash-Attention 2 and falls back gracefully.

### If Flash-Attention 2 installation fails
It's optional! The script will work without it (just slightly slower).

### To force re-download the model
```bash
rm -rf ~/.cache/huggingface/hub/models--Qwen--Qwen2.5-Omni-7B
```

## üéØ Why Qwen2.5-Omni Over Others?

‚úÖ **Better ASR**: 1.6/3.4 WER vs 1.6/3.6 (Qwen2-Audio)  
‚úÖ **Faster**: 7B vs 30B (Qwen3-Omni)  
‚úÖ **Memory Efficient**: ~14GB vs ~60GB  
‚úÖ **Official Benchmark**: Outperforms Qwen2-Audio on Common Voice  

## üìù Files Changed

1. ‚úÖ `evaluate.py` - Added Qwen2.5-Omni detection
2. ‚úÖ `transcribe/transcribe_qwen25omni.py` - New transcriber (NEW FILE)
3. ‚úÖ `README.md` - Added comprehensive documentation

## ‚ú® Next Steps

1. **Upload the 3 files** to your server
2. **(Optional)** Install Flash-Attention 2 for speed
3. **Clear old cached model** if needed
4. **Run evaluation** and enjoy better ASR! üéâ
