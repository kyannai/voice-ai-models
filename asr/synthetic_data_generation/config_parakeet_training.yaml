# Parakeet-TDT Training Configuration for Synthetic Data
# Continued training on Malaysian names and numbers

model:
  # Pre-trained model to continue training from
  name: "nvidia/parakeet-tdt-0.6b-v3"
  
  # Memory optimization
  gradient_checkpointing: true

data:
  # NeMo manifest files generated by prepare_nemo_manifest.py
  train_manifest: "./outputs/manifests/train_manifest.json"
  val_manifest: "./outputs/manifests/val_manifest.json"
  
  # Use all synthetic data
  max_samples: -1
  max_val_samples: -1
  
  # Audio processing
  sampling_rate: 22050  # ElevenLabs default (will be resampled to 16000 by NeMo if needed)
  max_audio_length: 30.0
  min_audio_length: 1.0

training:
  # Output directory
  output_dir: "./outputs/parakeet_training"
  run_name: "parakeet-tdt-synthetic-names-numbers"
  
  # Training epochs - MORE epochs for focused fine-tuning
  num_train_epochs: 5  # More epochs since dataset is focused
  max_steps: -1
  
  # Batch size (adjust based on GPU memory)
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 8  # Effective batch size = 8 * 8 = 64
  
  # Optimizer settings - LOWER learning rate for continued training
  optimizer: "adamw_8bit"  # Memory efficient
  learning_rate: 1.0e-5  # Lower LR for fine-tuning (10x lower than from scratch)
  weight_decay: 0.0001
  max_grad_norm: 1.0
  
  # Learning rate scheduler
  scheduler: "CosineAnnealing"
  warmup_steps: 100  # Shorter warmup for continued training
  min_learning_rate: 1.0e-7
  
  # Precision
  fp16: false
  bf16: true  # Recommended for A100/H100
  
  # Logging and evaluation
  logging_steps: 50
  eval_steps: 500  # Evaluate every 500 steps
  save_steps: 500  # Save every 500 steps
  save_total_limit: 5  # Keep last 5 checkpoints
  load_best_model_at_end: true
  metric_for_best_model: "val_wer"
  greater_is_better: false  # Lower WER is better
  
  # DataLoader settings
  dataloader_num_workers: 4
  dataloader_pin_memory: true
  
  # Resume from checkpoint
  resume_from_checkpoint: false
  # checkpoint_path: ""  # Set this to resume from a specific checkpoint
  
  # Hardware
  num_gpus: 1

# Weights & Biases (optional)
wandb:
  enabled: false
  project: "parakeet-tdt-synthetic"
  run_name: "names-numbers-malay"

# Notes:
# - This config is optimized for continued training on synthetic data
# - Lower learning rate prevents catastrophic forgetting
# - More epochs since the dataset is focused on specific tasks
# - Smaller batch size works well for focused fine-tuning
# - Monitor val_wer to ensure model is learning names and numbers
#
# Usage from train_parakeet_tdt folder:
# python train_parakeet_tdt.py --config ../synthetic_data_generation/config_parakeet_training.yaml

