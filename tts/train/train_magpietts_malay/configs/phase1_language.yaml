# Phase 1: Language Training Config
# ==================================
# Teaches MagpieTTS to understand Malay as a new language.
# Creates a Malay-capable base model with built-in G2P.
#
# Fresh training with reset Spanish embeddings -> Malay IPA vocab.
# Encoder/decoder are pretrained, embeddings are random initialized.
#
# Run: make phase1-train
# Output: models/malay_base.nemo

# Training phase identifier
phase: language

# Pretrained model to fine-tune
pretrained_model: nvidia/magpie_tts_multilingual_357m

# G2P dictionary path (generated by phase1-prepare)
g2p_dict: data/g2p/ipa_malay_dict.txt

# Model configuration
model:
  # Data paths
  train_manifest: data/manifests/train_manifest.json
  val_manifest: data/manifests/val_manifest.json
  
  # Training hyperparameters for FRESH LANGUAGE EMBEDDINGS
  # Conservative LR to protect pretrained encoder/decoder
  # Embeddings will still learn well with lower LR given 610k samples
  batch_size: 32             # Smaller batch for more gradient updates
  learning_rate: 1e-4        # Conservative for pretrained layers
  
  # Audio settings
  sample_rate: 22050
  
  # Train all layers - embeddings need to learn, encoder/decoder adapt
  freeze_encoder: false

# Trainer configuration
trainer:
  max_epochs: 20             # More epochs for fresh embeddings to converge
  precision: '16-mixed'
  accumulate_grad_batches: 4  # Effective batch = 128 (same throughput, more stable)
  gradient_clip_val: 0.5     # Tighter clipping for stability
  log_every_n_steps: 100
  val_check_interval: 0.25   # Check 4x per epoch for better monitoring
  strategy: auto

# Experiment manager
exp_manager:
  exp_dir: experiments/phase1_language
  name: malay_language_training
  
  create_checkpoint_callback: true
  checkpoint_callback_params:
    monitor: val_loss
    mode: min
    save_top_k: 5            # Keep more checkpoints to find best
    save_last: true
    filename: 'malay_lang-{epoch:02d}-{val_loss:.4f}'
  
  resume_if_exists: true
  resume_ignore_no_checkpoint: true
  
  create_tensorboard_logger: true
  create_wandb_logger: false

# Optimizer
optim:
  name: adamw
  lr: 1e-4                   # Match model.learning_rate
  betas: [0.9, 0.98]
  weight_decay: 0.01
  sched:
    name: CosineAnnealing
    warmup_steps: 2000       # ~10% of first epoch, gradual ramp-up
    min_lr: 1e-6
